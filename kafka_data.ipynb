{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent message: Key=0, Value=message number 0\n",
      "Sent message: Key=1, Value=message number 1\n",
      "Sent message: Key=2, Value=message number 2\n",
      "Sent message: Key=3, Value=message number 3\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "# Kafka broker address\n",
    "bootstrap_servers = 'kafka:9092'\n",
    "\n",
    "# Create a Kafka producer instance\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=bootstrap_servers,\n",
    "    key_serializer=str.encode,  # Key serializer\n",
    "    value_serializer=str.encode,  # Value serializer\n",
    ")\n",
    "\n",
    "# Topic to send messages to\n",
    "topic = 'example_topic'\n",
    "\n",
    "# Send messages with keys\n",
    "for i in range(0,4):\n",
    "    producer.send(topic, key=\"{}\".format(i), value=\"message number {}\".format(i))\n",
    "    print(f\"Sent message: Key={i}, Value=message number {i}\")\n",
    "\n",
    "# Close the producer\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message Types\n",
    "\n",
    "Kafka supports different payloads, let's have a look. Copy these examples into cells above your Consumer cell if you want to run the Kafka Consumer to see the output.\n",
    "\n",
    "String is what we've been using recently and with all messages you can send with or without a key.\n",
    "\n",
    "JSON is another type and very often used, here is an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "# Kafka broker address\n",
    "bootstrap_servers = 'kafka:9092'\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=bootstrap_servers,\n",
    "value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "\n",
    "\n",
    "# Topic to send messages to\n",
    "topic = 'example_topic'\n",
    "\n",
    "# Create a Python dictionary representing your message\n",
    "message = {\n",
    "'id': 1,\n",
    "'name': 'John Doe',\n",
    "'age': 30\n",
    "}\n",
    "\n",
    "# Send the message in JSON format\n",
    "producer.send(topic, value=message)\n",
    "\n",
    "# Close the producer\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell 1 - Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TopicAlreadyExistsError",
     "evalue": "[Error 36] TopicAlreadyExistsError: Request 'CreateTopicsRequest_v3(create_topic_requests=[(topic='avro_topic_test', num_partitions=1, replication_factor=1, replica_assignment=[], configs=[])], timeout=30000, validate_only=False)' failed with response 'CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='avro_topic_test', error_code=36, error_message=\"Topic 'avro_topic_test' already exists.\")])'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTopicAlreadyExistsError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m topic_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m topic_list\u001b[39m.\u001b[39mappend(NewTopic(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mavro_topic_test\u001b[39m\u001b[39m\"\u001b[39m, num_partitions\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, replication_factor\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m---> 11\u001b[0m admin_client\u001b[39m.\u001b[39;49mcreate_topics(new_topics\u001b[39m=\u001b[39;49mtopic_list, validate_only\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kafka/admin/client.py:461\u001b[0m, in \u001b[0;36mKafkaAdminClient.create_topics\u001b[0;34m(self, new_topics, timeout_ms, validate_only)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    457\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSupport for CreateTopics v\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m has not yet been added to KafkaAdminClient.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    458\u001b[0m         \u001b[39m.\u001b[39mformat(version))\n\u001b[1;32m    459\u001b[0m \u001b[39m# TODO convert structs to a more pythonic interface\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[39m# TODO raise exceptions if errors\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request_to_controller(request)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kafka/admin/client.py:407\u001b[0m, in \u001b[0;36mKafkaAdminClient._send_request_to_controller\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[39melif\u001b[39;00m error_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Errors\u001b[39m.\u001b[39mNoError:\n\u001b[0;32m--> 407\u001b[0m         \u001b[39mraise\u001b[39;00m error_type(\n\u001b[1;32m    408\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mRequest \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m failed with response \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m             \u001b[39m.\u001b[39mformat(request, response))\n\u001b[1;32m    410\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    411\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "\u001b[0;31mTopicAlreadyExistsError\u001b[0m: [Error 36] TopicAlreadyExistsError: Request 'CreateTopicsRequest_v3(create_topic_requests=[(topic='avro_topic_test', num_partitions=1, replication_factor=1, replica_assignment=[], configs=[])], timeout=30000, validate_only=False)' failed with response 'CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='avro_topic_test', error_code=36, error_message=\"Topic 'avro_topic_test' already exists.\")])'."
     ]
    }
   ],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "\n",
    "admin_client = KafkaAdminClient(\n",
    "    bootstrap_servers=\"kafka:9092\", \n",
    "    client_id='test'\n",
    ")\n",
    "\n",
    "topic_list = []\n",
    "topic_list.append(NewTopic(name=\"avro_topic_test\", num_partitions=1, replication_factor=1))\n",
    "admin_client.create_topics(new_topics=topic_list, validate_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell 2 - producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kafka.producer.future.FutureRecordMetadata at 0xffff6abda2d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "from avro import schema, io\n",
    "import avro.schema\n",
    "import avro.io \n",
    "from io import BytesIO\n",
    "\n",
    "# Configure Kafka producer\n",
    "bootstrap_servers = 'kafka:9092'  # Update with your Kafka broker address\n",
    "topic = 'avro_topic_test'\n",
    "\n",
    "# Load Avro schema from a file or define it programmatically\n",
    "avro_schema = avro.schema.parse(open('./work/user.avsc', 'rb').read())\n",
    "\n",
    "\n",
    "# Create a Python dictionary representing your message\n",
    "message = {\n",
    "    'first_name': 'John',\n",
    "    'last_name': 'Doe',\n",
    "    'age': 30\n",
    "}\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=bootstrap_servers,\n",
    "    # key_serializer=str.encode,  # Key serializer\n",
    "    # value_serializer=str.encode,  # Value serializer\n",
    ")\n",
    "\n",
    "avro_bytes_writer = BytesIO()\n",
    "avro_writer = io.DatumWriter(avro_schema)\n",
    "encoder = io.BinaryEncoder(avro_bytes_writer)\n",
    "avro_writer.write(message, encoder)\n",
    "avro_bytes = avro_bytes_writer.getvalue()\n",
    "\n",
    "# Send the Avro message to Kafka topic\n",
    "producer.send(topic=topic, value=avro_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell 3 - consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n",
      "{'first_name': 'John', 'last_name': 'Doe', 'age': 30}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m      8\u001b[0m consumer \u001b[39m=\u001b[39m KafkaConsumer(\n\u001b[1;32m      9\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mavro_topic_test\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     bootstrap_servers\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mkafka:9092\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m     group_id\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtester\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m     auto_offset_reset\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mearliest\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m avro_schema \u001b[39m=\u001b[39m avro\u001b[39m.\u001b[39mschema\u001b[39m.\u001b[39mparse(\u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m./work/user.avsc\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mread())\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfor\u001b[39;00m message \u001b[39min\u001b[39;00m consumer:\n\u001b[1;32m     18\u001b[0m     \u001b[39m# Process the Avro message\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     avro_reader \u001b[39m=\u001b[39m DatumReader(avro_schema)\n\u001b[1;32m     20\u001b[0m     avro_bytes_reader \u001b[39m=\u001b[39m BytesIO(message\u001b[39m.\u001b[39mvalue)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kafka/consumer/group.py:1193\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_v1()\n\u001b[1;32m   1192\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_v2()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kafka/consumer/group.py:1201\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_generator_v2()\n\u001b[1;32m   1200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1201\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator)\n\u001b[1;32m   1202\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kafka/consumer/group.py:1116\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_message_generator_v2\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1115\u001b[0m     timeout_ms \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consumer_timeout \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mtime())\n\u001b[0;32m-> 1116\u001b[0m     record_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpoll(timeout_ms\u001b[39m=\u001b[39;49mtimeout_ms, update_offsets\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m   1117\u001b[0m     \u001b[39mfor\u001b[39;00m tp, records \u001b[39min\u001b[39;00m six\u001b[39m.\u001b[39miteritems(record_map):\n\u001b[1;32m   1118\u001b[0m         \u001b[39m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m         \u001b[39m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m         \u001b[39m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m         \u001b[39mfor\u001b[39;00m record \u001b[39min\u001b[39;00m records:\n\u001b[1;32m   1122\u001b[0m             \u001b[39m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m             \u001b[39m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m             \u001b[39m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m             \u001b[39m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kafka/consumer/group.py:655\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    653\u001b[0m remaining \u001b[39m=\u001b[39m timeout_ms\n\u001b[1;32m    654\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     records \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll_once(remaining, max_records, update_offsets\u001b[39m=\u001b[39;49mupdate_offsets)\n\u001b[1;32m    656\u001b[0m     \u001b[39mif\u001b[39;00m records:\n\u001b[1;32m    657\u001b[0m         \u001b[39mreturn\u001b[39;00m records\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kafka/consumer/group.py:702\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mpoll(timeout_ms\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    701\u001b[0m timeout_ms \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(timeout_ms, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coordinator\u001b[39m.\u001b[39mtime_to_next_poll() \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m)\n\u001b[0;32m--> 702\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mpoll(timeout_ms\u001b[39m=\u001b[39;49mtimeout_ms)\n\u001b[1;32m    703\u001b[0m \u001b[39m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[39m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coordinator\u001b[39m.\u001b[39mneed_rejoin():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kafka/client_async.py:602\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    599\u001b[0m             timeout \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(timeout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mretry_backoff_ms\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    600\u001b[0m         timeout \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, timeout)  \u001b[39m# avoid negative timeouts\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout \u001b[39m/\u001b[39;49m \u001b[39m1000\u001b[39;49m)\n\u001b[1;32m    604\u001b[0m \u001b[39m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[39m# if handlers need to acquire locks\u001b[39;00m\n\u001b[1;32m    606\u001b[0m responses\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kafka/client_async.py:634\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_register_send_sockets()\n\u001b[1;32m    633\u001b[0m start_select \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 634\u001b[0m ready \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    635\u001b[0m end_select \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sensors:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/selectors.py:468\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    467\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_selector\u001b[39m.\u001b[39mpoll(timeout, max_ev)\n\u001b[1;32m    469\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from avro.io import DatumReader, BinaryDecoder\n",
    "from io import BytesIO\n",
    "import avro.schema\n",
    "import io\n",
    "\n",
    "# Create a Kafka consumer instance\n",
    "consumer = KafkaConsumer(\n",
    "    'avro_topic_test',\n",
    "    bootstrap_servers='kafka:9092',\n",
    "    group_id='tester',\n",
    "    auto_offset_reset='earliest'\n",
    ")\n",
    "\n",
    "avro_schema = avro.schema.parse(open('./work/user.avsc', 'rb').read())\n",
    "\n",
    "for message in consumer:\n",
    "    # Process the Avro message\n",
    "    avro_reader = DatumReader(avro_schema)\n",
    "    avro_bytes_reader = BytesIO(message.value)\n",
    "    decoder = BinaryDecoder(avro_bytes_reader)\n",
    "    decoded_message = avro_reader.read(decoder)\n",
    "    print(\"working\")\n",
    "    # print(message)\n",
    "    print(decoded_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'my_protobuf_pb2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkafka\u001b[39;00m \u001b[39mimport\u001b[39;00m KafkaProducer\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmy_protobuf_pb2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Configure Kafka producer\u001b[39;00m\n\u001b[1;32m      5\u001b[0m bootstrap_servers \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlocalhost:9092\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# Update with your Kafka broker address\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'my_protobuf_pb2'"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "import my_protobuf_pb2\n",
    "\n",
    "# Configure Kafka producer\n",
    "bootstrap_servers = 'localhost:9092'  # Update with your Kafka broker address\n",
    "topic = 'my_topic'\n",
    "\n",
    "# Create Kafka producer instance\n",
    "producer = KafkaProducer(bootstrap_servers=bootstrap_servers,\n",
    "                        value_serializer=lambda v: v.SerializeToString())\n",
    "\n",
    "# Create a Protobuf message using your defined schema\n",
    "message = my_protobuf_pb2.MyMessage()\n",
    "message.id = 1\n",
    "message.name = 'John Doe'\n",
    "message.age = 30\n",
    "\n",
    "# Send the message in Protobuf format\n",
    "producer.send(topic, value=message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consuming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 1, \"name\": \"John Doe\", \"age\": 30}\n",
      "message number 0\n",
      "message number 1\n",
      "message number 2\n",
      "message number 3\n",
      "{\"id\": 1, \"name\": \"John Doe\", \"age\": 30}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m consumer \u001b[39m=\u001b[39m KafkaConsumer(\n\u001b[1;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mexample_topic\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     bootstrap_servers\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mkafka:9092\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     group_id\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmy_consumer_group\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     auto_offset_reset\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mearliest\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[39m# Continuously poll for new messages\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfor\u001b[39;00m message \u001b[39min\u001b[39;00m consumer:\n\u001b[1;32m     13\u001b[0m     \u001b[39m# Decode the message value assuming it's in bytes\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     message_value \u001b[39m=\u001b[39m message\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[39mprint\u001b[39m(message_value)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kafka/consumer/group.py:1193\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_v1()\n\u001b[1;32m   1192\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_v2()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kafka/consumer/group.py:1201\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_generator_v2()\n\u001b[1;32m   1200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1201\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator)\n\u001b[1;32m   1202\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kafka/consumer/group.py:1116\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_message_generator_v2\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1115\u001b[0m     timeout_ms \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consumer_timeout \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mtime())\n\u001b[0;32m-> 1116\u001b[0m     record_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpoll(timeout_ms\u001b[39m=\u001b[39;49mtimeout_ms, update_offsets\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m   1117\u001b[0m     \u001b[39mfor\u001b[39;00m tp, records \u001b[39min\u001b[39;00m six\u001b[39m.\u001b[39miteritems(record_map):\n\u001b[1;32m   1118\u001b[0m         \u001b[39m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m         \u001b[39m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m         \u001b[39m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m         \u001b[39mfor\u001b[39;00m record \u001b[39min\u001b[39;00m records:\n\u001b[1;32m   1122\u001b[0m             \u001b[39m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m             \u001b[39m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m             \u001b[39m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m             \u001b[39m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kafka/consumer/group.py:655\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    653\u001b[0m remaining \u001b[39m=\u001b[39m timeout_ms\n\u001b[1;32m    654\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     records \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll_once(remaining, max_records, update_offsets\u001b[39m=\u001b[39;49mupdate_offsets)\n\u001b[1;32m    656\u001b[0m     \u001b[39mif\u001b[39;00m records:\n\u001b[1;32m    657\u001b[0m         \u001b[39mreturn\u001b[39;00m records\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kafka/consumer/group.py:702\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mpoll(timeout_ms\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    701\u001b[0m timeout_ms \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(timeout_ms, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coordinator\u001b[39m.\u001b[39mtime_to_next_poll() \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m)\n\u001b[0;32m--> 702\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mpoll(timeout_ms\u001b[39m=\u001b[39;49mtimeout_ms)\n\u001b[1;32m    703\u001b[0m \u001b[39m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[39m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coordinator\u001b[39m.\u001b[39mneed_rejoin():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kafka/client_async.py:602\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    599\u001b[0m             timeout \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(timeout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mretry_backoff_ms\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    600\u001b[0m         timeout \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, timeout)  \u001b[39m# avoid negative timeouts\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout \u001b[39m/\u001b[39;49m \u001b[39m1000\u001b[39;49m)\n\u001b[1;32m    604\u001b[0m \u001b[39m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[39m# if handlers need to acquire locks\u001b[39;00m\n\u001b[1;32m    606\u001b[0m responses\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kafka/client_async.py:634\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_register_send_sockets()\n\u001b[1;32m    633\u001b[0m start_select \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 634\u001b[0m ready \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    635\u001b[0m end_select \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sensors:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/selectors.py:468\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    467\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_selector\u001b[39m.\u001b[39mpoll(timeout, max_ev)\n\u001b[1;32m    469\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "# Create a Kafka consumer instance\n",
    "consumer = KafkaConsumer(\n",
    "    'example_topic',\n",
    "    bootstrap_servers='kafka:9092',\n",
    "    group_id='my_consumer_group',\n",
    "    auto_offset_reset='earliest'\n",
    ")\n",
    "\n",
    "# Continuously poll for new messages\n",
    "for message in consumer:\n",
    "    # Decode the message value assuming it's in bytes\n",
    "    message_value = message.value.decode('utf-8')\n",
    "    print(message_value)\n",
    "\n",
    "# Close the consumer connection\n",
    "consumer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
