{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Topic\n",
    "\n",
    "Let's create a topic called: monster_movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaAdminClient\n",
    "from kafka.admin import NewTopic\n",
    "\n",
    "# Kafka broker configuration\n",
    "bootstrap_servers = \"kafka:9092\"\n",
    "\n",
    "# Create an instance of KafkaAdminClient\n",
    "admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers)\n",
    "\n",
    "# Define the topics to be created\n",
    "topics = [\n",
    "    NewTopic(name=\"monster_movements\", num_partitions=1, replication_factor=1)\n",
    "]\n",
    "\n",
    "# Create the topics\n",
    "admin_client.create_topics(new_topics=topics)\n",
    "\n",
    "# Close the admin client\n",
    "admin_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "Next we'll generate the data. We want to use the functions we create in our Spark streaming practice examples. What we want to achieve is:\n",
    "\n",
    "1. Create 200 events.\n",
    "2. The event should pick five random rows from our dnd_monsters.csv and return the monster's name and str characteristic.\n",
    "3. Along with the name and str we need the latitude and longitude of the monster and a timestamp. Latitude is between -90 and 90 and longitude is -180 to 180.\n",
    "4. Add a timestamp field called ts.\n",
    "5. Remember to put everything into JSON then into a field called value for the kafka message.\n",
    "5. Don't write to Kafka yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, current_timestamp\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Kafka Monsters\").getOrCreate()\n",
    "\n",
    "# Read data from the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"./work/data/dnd_monsters.csv\",  header=True)\n",
    "\n",
    "# Define the number of events to generate\n",
    "num_of_events = 200\n",
    "\n",
    "# Select random rows and extract required fields (name, strength)\n",
    "random_monsters = df.orderBy(rand()).limit(num_of_events).select(\"name\", \"str\")\n",
    "\n",
    "# Add latitude and longitude to DataFrame\n",
    "random_monsters = random_monsters.withColumn(\"lat\", rand() * 180 - 90)\n",
    "random_monsters = random_monsters.withColumn(\"long\", rand() * 360 - 180)\n",
    "\n",
    "# Add a timestamp field to each row\n",
    "random_monsters = random_monsters.withColumn(\"ts\", current_timestamp())\n",
    "\n",
    "# Format the data into JSON and store in a new column\n",
    "formatted_monsters = random_monsters.toJSON().map(json.loads)\n",
    "\n",
    "# Show the data\n",
    "for monsters in formatted_monsters.collect():\n",
    "    print(monsters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add more\n",
    "\n",
    "Now that we have our JSON monster locations printing and ready to use, but first let's change those latitudes and longitudes to countries. Again try this yourself first and don't give up too quickly if you find it hard, use the internet. Here's the library and how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopy\n",
      "  Downloading geopy-2.3.0-py3-none-any.whl (119 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting geographiclib<3,>=1.52 (from geopy)\n",
      "  Downloading geographiclib-2.0-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-2.0 geopy-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United Kingdom\n",
      "United Kingdom\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Create a geocoder instance\n",
    "geolocator = Nominatim(user_agent=\"my_geocoder\")\n",
    "\n",
    "# Define the latitude and longitude coordinates\n",
    "latitude = 51.5074\n",
    "longitude = -0.1278\n",
    "\n",
    "# Reverse geocode to get the location information\n",
    "location = geolocator.reverse((latitude, longitude), language='en')\n",
    "\n",
    "# Extract the country from the location information, with a random lat and long it might not have a country\n",
    "country = \"Sea\"\n",
    "try:\n",
    "    country = location.raw['address']['country']\n",
    "except:\n",
    "    print(\"out to sea\")\n",
    "print(country)\n",
    "\n",
    "# Print the country\n",
    "print(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|name        |str |\n",
      "+------------+----+\n",
      "|lich        |11.0|\n",
      "|steam-mephit|5.0 |\n",
      "|bone-devil  |18.0|\n",
      "|ghast       |16.0|\n",
      "|merfolk     |10.0|\n",
      "+------------+----+\n",
      "\n",
      "out to sea\n",
      "Sea\n",
      "{\"name\": \"steam-mephit\", \"str\": \"5.0\", \"ts\": \"2023-08-01 13:30:43.320865\", \"lat\": -42, \"long\": -44, \"country\": \"Sea\"}\n",
      "+---------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                |\n",
      "+---------------------------------------------------------------------------------------------------------------------+\n",
      "|{\"name\": \"steam-mephit\", \"str\": \"5.0\", \"ts\": \"2023-08-01 13:30:43.320865\", \"lat\": -42, \"long\": -44, \"country\": \"Sea\"}|\n",
      "+---------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 69\u001b[0m\n\u001b[1;32m     63\u001b[0m df\u001b[39m.\u001b[39mshow(\u001b[39m1\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     64\u001b[0m \u001b[39m# # Write the DataFrame to Kafka\u001b[39;00m\n\u001b[1;32m     65\u001b[0m df\u001b[39m.\u001b[39;49mwrite \\\n\u001b[1;32m     66\u001b[0m     \u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mkafka\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m     67\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mkafka.bootstrap.servers\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mkafka:9092\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m     68\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mtopic\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mmonster_movements\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m---> 69\u001b[0m     \u001b[39m.\u001b[39;49msave()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1396\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1394\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mformat\u001b[39m)\n\u001b[1;32m   1395\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1396\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave()\n\u001b[1;32m   1397\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1398\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand\n",
    "import random\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Create a geocoder instance\n",
    "geolocator = Nominatim(user_agent=\"my_geocoder\")\n",
    "\n",
    "\n",
    "def choose_random_item(lst):\n",
    "    return random.choice(lst)\n",
    "\n",
    "def random_int(fro, to):\n",
    "    return random.randint(fro, to)\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Monsters\") \\\n",
    ".config(\"spark.jars\",\"commons-pool2-2.11.1.jar,spark-sql-kafka-0-10_2.12-3.4.0.jar,spark-streaming-kafka-0-10-assembly_2.12-3.4.0.jar\").getOrCreate()\n",
    "\n",
    "# Read the file into a DataFrame\n",
    "df = spark.read.csv(\"./work/data/dnd_monsters.csv\",  header=True).where(\"str is not null\").select(\"name\", \"str\")\n",
    "\n",
    "# Randomly select 5 lines\n",
    "random_lines = df.orderBy(rand()).limit(5)\n",
    "\n",
    "# Show the selected lines\n",
    "random_lines.show(truncate=False)\n",
    "list_tuples = random_lines.rdd.map(tuple).collect()\n",
    "\n",
    "\n",
    "for monster in range(1,200):\n",
    "    random_tuple = random.choice(list_tuples)\n",
    "    lat = random_int(-90, 90)\n",
    "    long = random_int(-180, 180)\n",
    "\n",
    "    # Reverse geocode to get the location information\n",
    "    location = geolocator.reverse((lat, long), language='en')\n",
    "\n",
    "    # Extract the country from the location information\n",
    "    country = \"Sea\"\n",
    "    try:\n",
    "        country = location.raw['address']['country']\n",
    "    except:\n",
    "        print(\"out to sea\")\n",
    "    print(country)\n",
    "    data = {\n",
    "    \"name\": random_tuple[0],\n",
    "    \"str\": random_tuple[1],\n",
    "    \"ts\": str(datetime.datetime.now()),\n",
    "    \"lat\": lat,\n",
    "    \"long\": long,\n",
    "    \"country\": country\n",
    "    }\n",
    "\n",
    "    json_string = json.dumps(data)\n",
    "\n",
    "    # Print the JSON string\n",
    "    print(json_string)\n",
    "    data = [(json_string,),]\n",
    "    df = spark.createDataFrame(data, [\"value\"])\n",
    "    df.show(1, False)\n",
    "    # # Write the DataFrame to Kafka\n",
    "    df.write \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "        .option(\"topic\", \"monster_movements\") \\\n",
    "        .save()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 67\u001b[0m\n\u001b[1;32m     59\u001b[0m final_data \u001b[39m=\u001b[39m random_monsters\u001b[39m.\u001b[39mselect(\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlat\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlong\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mts\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcountry\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[39m# Write the DataFrame to Kafka\u001b[39;00m\n\u001b[1;32m     62\u001b[0m final_data\u001b[39m.\u001b[39;49mselectExpr(\u001b[39m\"\u001b[39;49m\u001b[39mCAST(name AS STRING) AS key\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mto_json(struct(*)) AS value\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m     63\u001b[0m     \u001b[39m.\u001b[39;49mwrite \\\n\u001b[1;32m     64\u001b[0m     \u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mkafka\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m     65\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mkafka.bootstrap.servers\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mkafka:9092\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m     66\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mtopic\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mmonster_movements\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m---> 67\u001b[0m     \u001b[39m.\u001b[39;49msave()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1396\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1394\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mformat\u001b[39m)\n\u001b[1;32m   1395\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1396\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave()\n\u001b[1;32m   1397\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1398\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, current_timestamp, udf\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Create a geocoder instance\n",
    "geolocator = Nominatim(user_agent=\"my_geocoder\")\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Kafka Monsters\").getOrCreate()\n",
    "\n",
    "# Read data from the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"./work/data/dnd_monsters.csv\", header=True)\n",
    "\n",
    "# Define the number of events to generate\n",
    "num_of_events = 200\n",
    "\n",
    "# Select random rows and extract required fields (name, strength)\n",
    "random_monsters = df.orderBy(rand()).limit(num_of_events).select(\"name\", \"str\")\n",
    "\n",
    "# Add latitude and longitude to DataFrame\n",
    "random_monsters = random_monsters.withColumn(\"lat\", rand() * 180 - 90)\n",
    "random_monsters = random_monsters.withColumn(\"long\", rand() * 360 - 180)\n",
    "\n",
    "# Add a timestamp field to each row\n",
    "random_monsters = random_monsters.withColumn(\"ts\", current_timestamp())\n",
    "\n",
    "# Convert latitude and longitude to string\n",
    "random_monsters = random_monsters.withColumn(\"lat_str\", random_monsters[\"lat\"].cast(StringType()))\n",
    "random_monsters = random_monsters.withColumn(\"long_str\", random_monsters[\"long\"].cast(StringType()))\n",
    "\n",
    "# Format the data into JSON and store in a new column\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"str\", StringType(), True),\n",
    "    StructField(\"lat\", StringType(), True),\n",
    "    StructField(\"long\", StringType(), True),\n",
    "    StructField(\"ts\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "def get_country(lat, long):\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, long), language='en')\n",
    "        country = location.raw['address']['country']\n",
    "        return country\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "get_country_udf = udf(get_country, StringType())\n",
    "\n",
    "# Add the country field to the DataFrame\n",
    "random_monsters = random_monsters.withColumn(\"country\", get_country_udf(\"lat_str\", \"long_str\"))\n",
    "\n",
    "# Select the required columns for the final DataFrame\n",
    "final_data = random_monsters.select(\"name\", \"str\", \"lat\", \"long\", \"ts\", \"country\")\n",
    "\n",
    "# Write the DataFrame to Kafka\n",
    "final_data.selectExpr(\"CAST(name AS STRING) AS key\", \"to_json(struct(*)) AS value\") \\\n",
    "    .write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"topic\", \"monster_movements\") \\\n",
    "    .save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
